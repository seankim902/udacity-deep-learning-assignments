{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 97\n",
      "Unexpected character: ï\n",
      "(1, 26, 0, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print vocabulary_size, first_letter\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0)), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._text_size, batch_size, segment 99999000 64 1562484\n",
      "self._cursor [0, 1562484, 3124968, 4687452, 6249936, 7812420, 9374904, 10937388, 12499872, 14062356, 15624840, 17187324, 18749808, 20312292, 21874776, 23437260, 24999744, 26562228, 28124712, 29687196, 31249680, 32812164, 34374648, 35937132, 37499616, 39062100, 40624584, 42187068, 43749552, 45312036, 46874520, 48437004, 49999488, 51561972, 53124456, 54686940, 56249424, 57811908, 59374392, 60936876, 62499360, 64061844, 65624328, 67186812, 68749296, 70311780, 71874264, 73436748, 74999232, 76561716, 78124200, 79686684, 81249168, 82811652, 84374136, 85936620, 87499104, 89061588, 90624072, 92186556, 93749040, 95311524, 96874008, 98436492]\n",
      "self._text_size, batch_size, segment 1000 1 1000\n",
      "self._cursor [0]\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    print 'self._text_size, batch_size, segment', self._text_size, batch_size, segment\n",
    "    print 'self._cursor', self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294055 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "bxml  ngjh ileekua roiao m dqyrlh qsixeaaa pvrmdfrvaxtvnp fh  xn eti  poderc ave\n",
      "oportisirdfttd gjnfaazqvhfiftrs qeg kvrmceigorvekcsoviqfoeaqmglic yhspvppdepwndx\n",
      "shbeu qiguonitajcsaupiekz  mpaivveaohie dwy eshymupseqmfbyzisuks jnq aiatirnpjeg\n",
      "qg lrvhpiwfowdfibpmqzpz ubjnrm veydldw mm zes  mx nsodeptwx qtivn xvvairrwwfmscn\n",
      "rirapm zeegmawjsvnc ziv epgccesn qenne qdem urv etf ms   lhprhraaertraizlwzbdaop\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100: 2.598098 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.90\n",
      "Validation set perplexity: 10.30\n",
      "Average loss at step 200: 2.261679 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.60\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.103733 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.001989 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 500: 1.939416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.911046 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 700: 1.856221 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 800: 1.817907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.826209 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 1000: 1.824098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "ded the lore form commune of fomeral fro soudh of appreleaden import punifing th\n",
      " in one nion be relectional hehtce norph one nine oberwerve from use statative a\n",
      "fiectural of cinyelag outs stolobail ty be town oppenconnt corfor becix in anose\n",
      "ements fletul of forse in reeations evemeny his plile of lade well conkolity chr\n",
      "o sime of the pa rectray sometered tex two of phele whet the he boldowilifial of\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.773923 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.751284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.727107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.744279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1500: 1.730483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1600: 1.742478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1700: 1.706335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1800: 1.670282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 1900: 1.641154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2000: 1.690038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      " be boochiles vives a maptord slive tepo is in comparce of misternon conkind put\n",
      "s tite the mud frod censing lep ider fatter is durals lapling as the eneur in th\n",
      "pbirila polall themen and s s nalfed mosited sclibor was the resm decrisentivily\n",
      "relw mobocity at looce near stast ii spake with was boild two faurly opedanation\n",
      "f indebrays in is on the productign sligp of towhtratrant as languass grips the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2100: 1.680259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2200: 1.671210 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2300: 1.636536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2400: 1.652792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2500: 1.674087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2600: 1.647072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2700: 1.650342 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2800: 1.643466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2900: 1.644461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000: 1.643784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "foraz forming vish for two obbues fasic harned the systems an juold proveded thr\n",
      "us it called then towhining and dead ba blawch fulk deftemull recoldsation was a\n",
      "ultel at chlogedry both callim becheraungle moy of staffation in jeffil firser n\n",
      "wn saclusedomin anarchisfical ge the lotor rodic gal leaded liminase the futeriz\n",
      "ble wastau includes and eprivilance to maderiageen onely certersabunts shows the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3100: 1.624354 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3200: 1.643500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3300: 1.631800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3400: 1.665900 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3500: 1.655417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3600: 1.665618 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3700: 1.642808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 3800: 1.641369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3900: 1.636452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.652282 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "d of jin lashinit examplip amobara peepury blackinct frepent in s goverard and e\n",
      "jus production comprotitary with house syre in hissele to delawimit on lectle it\n",
      "jured delational minived his king homologe six evarted reeis onve cacditiculy  s\n",
      "yle film indian as americms ecdian nound indoce beltwine beginds experson constr\n",
      "joent christian arring mastay arther a deyapkinor etheman c cay spide the smatau\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4100: 1.629328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4200: 1.629193 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4300: 1.608806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.605166 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4500: 1.610075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.613289 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4700: 1.623066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.629377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4900: 1.630228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5000: 1.602450 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "================================================================================\n",
      "ving a three zero zero zero zero zero zero zero zero minias is lecide of the say\n",
      "quere protes music plasebt one six to l adding his janned sword reschinoty lyse \n",
      "z has pajes most minusions hadfer pyath uptols bitologe no oken of the mussed lo\n",
      "pensian worsocide marime espanish by concession hamed offects by ava buly his wi\n",
      "ine since are sention of the sincom one nine eight six nine zero zero zero ial t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5100: 1.603589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5200: 1.591277 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5300: 1.573084 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5400: 1.575426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5500: 1.567384 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5600: 1.579059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5700: 1.564556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5800: 1.580981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5900: 1.572235 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6000: 1.542290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "nuntias young e obposics astrobmang one nine nine six eight eight zero zero ones\n",
      "valuad the is a pup yost newly major other the one sho as a two for purg bachoms\n",
      "t to this group partic came starting mital s exmantal relividatram the which eve\n",
      "row agous becausebic yover partinal controvoclims have international be autist w\n",
      "zing that amish event a supp rules be made they boly in eight untro unimal ingo \n",
      "================================================================================\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6100: 1.563320 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6200: 1.534163 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6300: 1.545558 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6400: 1.536813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6500: 1.554234 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6600: 1.595661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6700: 1.580991 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6800: 1.604422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6900: 1.577442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 7000: 1.569537 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "ble notto larging americar impreasional fance mar misions and ire greepic to iss\n",
      "shing harges and indeside for the calmged in resurve puller to two invige of pro\n",
      " sley this and eyer in some bachball article of for exajancemed in poshus wased \n",
      "resser leasuradity crusions she may as a resulters record the god therefogy of b\n",
      "y parilism english three s far eaculajed to britis of death happers and primatio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.08\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### [<tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>]\n",
      "####### [<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input, Forget, Memory, Output gate: input, previous output, and bias.\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  def _slice(_x, n, dim):\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \n",
    "    ifco_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    \n",
    "    input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n",
    "    forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n",
    "    update = _slice(ifco_gates, 2, num_nodes)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  print '#######',train_inputs\n",
    "  print '#######',train_labels\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296723 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "ta  st et qrdriyiamtxt  u hreewcgew yx  n owntn nt  daxpuew yetu jfi mmfdr tfe t\n",
      "wvlv a aq pnkef b kketkxqs be wbashphd emarasbflsi eec anrxcyqsqs urf t cor b  e\n",
      "f  ysxtpt  f tmtareoztiryji zw tbsna  hxmh dwnce tstsjsdrli asomewtto rxyirsjaiq\n",
      "ot xbadc jxfoexseg jpye tgkj  ua ctvgdpija bbiudeba inyhg cfyio xoeod cnihrr i  \n",
      "ju y nterefdbrrgizetzwkxqhdpryfngyheeiy  emptekeve nosreqql iblor sbyevisuntt n \n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.594855 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.66\n",
      "Validation set perplexity: 10.53\n",
      "Average loss at step 200: 2.251248 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 9.01\n",
      "Average loss at step 300: 2.084573 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 2.030528 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 500: 1.976125 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 600: 1.890144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.866905 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 800: 1.865970 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 900: 1.844291 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 1000: 1.842251 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "================================================================================\n",
      "s kast insums prestard on the vorkent cuarin ongchit geagp adfor as gamobo owsen\n",
      "ter on events of myldence systects in tionari sude playeres the firsddon of yust\n",
      "x in one nine a sanelal lemias and ithih miser by contrah arming sorne monthreye\n",
      "j see strue one nine eight three nite weitosy sowed kid olchisfore as to in antr\n",
      "d to kuse mome chovitioss resuge in eadiny burgh eturan witt flef laions in pudi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.798871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1200: 1.769404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1300: 1.758581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1400: 1.764511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.749986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.732933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1700: 1.716306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.692561 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.694299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2000: 1.678222 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "tas to retern olthers of the act citi will exterm to a times cunclitude estated \n",
      "ly in that the zero fext plale fild base and conrell essexple is adtering fitiss\n",
      "th hawen aim is the bankin grandal two zero zero four lea playently in becturl s\n",
      "ch uns lood eamon to gelectury to a sensement stitly sity such on the crease of \n",
      "vin sex where and viies polination in that hive p the trade pultlic lan na elect\n",
      "================================================================================\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2100: 1.686890 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2200: 1.703584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300: 1.706031 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2400: 1.687115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2500: 1.694855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2600: 1.672125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2700: 1.686917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2800: 1.682260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2900: 1.676656 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3000: 1.683663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "fren of testies of the celtzes from mayles gaulx subtar one nine two chames gepa\n",
      "zer to pugmy trace travine dausing it change also the oril otxtroxe extemman pex\n",
      "reses are to firsligens accesses informugs a or macgle telm specistions is ntill\n",
      "e plays freat to exampra lavia operated the othy used the to internologue is the\n",
      " oxtrog and time howevindov dom england to itya one nine six zero one six eight \n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.653249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3200: 1.638297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3300: 1.649814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3400: 1.639714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3500: 1.676634 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3600: 1.656612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3700: 1.655745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3800: 1.657898 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.653194 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4000: 1.639488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "justity which one nine seven one emalty incradder day the straminist from untoma\n",
      "den while also elity about the unders the poersen notaking disseves h j feuro on\n",
      "que from form veried indepleven and vision lefferatlan booker have the surving a\n",
      "wer and found the rephoses eursys miscust of fox the preglition daynia a p a dec\n",
      "king fectan of becausone imperian broucd one nine four four hid the mentable low\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.620749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.614014 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4300: 1.617734 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.607838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4500: 1.643875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4600: 1.626620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4700: 1.626753 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4800: 1.608520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4900: 1.619688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.614900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "grop four yearss of the is sea seven six one s alludents galment of the new lago\n",
      "ad of oucnush first in its loud of loublandan zee citereds nottber four commons \n",
      "theringher a neugral and world welfecuetion for seading weatye two zero conalute\n",
      "way has by their the six one six two recorded of maces life mcjposs and the and \n",
      "er on been comolivay ii the hell heingald the fresion have world boeglo a the km\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5100: 1.591364 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.595745 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.598895 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.594937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5500: 1.593662 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.561433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5700: 1.577282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.601914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.583060 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6000: 1.589158 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      " mean peatroup remagn current letopid thoukly resell song one three zero nine ar\n",
      " milectar activited by trate what are actresads larges in the australy than bug \n",
      "kingly callinas one the with the order the stook costrive the margeeloration tha\n",
      "d to asparenate creviit the example countic govern and his regning vardins compl\n",
      "vers the called and investive used in one nine pidents institutes and possiped t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6100: 1.576963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.591076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6300: 1.589913 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6400: 1.575683 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.562431 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.601211 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.571536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.577212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6900: 1.573910 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000: 1.592177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "fied cractive states broude expecial contenval achiam shavidion in the tuch mini\n",
      "dassian org and and is have loce side light nigends ticalalys and forsitias as t\n",
      "k the live this availly of leepion have and sabrigetwes a heombourr publice in a\n",
      "vet these diswornes as larges to accorded to and been american stake hiavori sor\n",
      "chablitryai de as stricks general ruth was inspited to cail chile he hou geound \n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "(31, 708, 0, 486, 0)\n",
      "('ad', 'zf', '  ', 'r ')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = (len(string.ascii_lowercase) + 1 )*(len(string.ascii_lowercase) + 1) # [a-z] + ' '\n",
    "\n",
    "idx2bi={}\n",
    "bi2idx={}\n",
    "idx = 0\n",
    "for i in ' '+string.ascii_lowercase:\n",
    "    for j in ' '+string.ascii_lowercase:\n",
    "        idx2bi[idx]=i+j\n",
    "        bi2idx[i+j]=idx\n",
    "        idx = idx + 1\n",
    "\n",
    "def bi2id(char):\n",
    "  if char in bi2idx.keys():\n",
    "    return bi2idx[char]\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2bi(dictid):\n",
    "  if dictid >= 0 and dictid < len(idx2bi):\n",
    "    return idx2bi[dictid]\n",
    "  else:\n",
    "    return '  '\n",
    "\n",
    "print(bi2id('ad'), bi2id('zf'), bi2id('  '),bi2id('r '), bi2id('ï'))\n",
    "print(id2bi(31), id2bi(708), id2bi(0),  id2bi(486) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class bi_BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    #print 'self._text_size, batch_size, segment', self._text_size, batch_size, segment\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    #print self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      \n",
    "      batch[b, bi2id(self._text[self._cursor[b]:self._cursor[b]+2])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2bi(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = bi_BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = bi_BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### [<tf.Tensor 'Placeholder:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 729) dtype=float32>]\n",
      "####### [<tf.Tensor 'Placeholder_1:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'Placeholder_10:0' shape=(64, 729) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input, Forget, Memory, Output gate: input, previous output, and bias.\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  def _slice(_x, n, dim):\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \n",
    "    ifco_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    \n",
    "    input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n",
    "    forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n",
    "    update = _slice(ifco_gates, 2, num_nodes)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  print '#######',train_inputs\n",
    "  print '#######',train_labels\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.592365 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.50\n",
      "================================================================================\n",
      "rwesrunb abudrffjkqqzrmzjushvxgkwhispydnqds smlutpsepbmoxlgugjpqrfutnuxviytvdfxsxomgrxzixxscltplzj prkijrlfkxxmzindvntbjcqivsadvrznvpogimjhmppsznscxktvuseschdtk\n",
      "h sgrpyxntzoewhvewyvxssqnnrpeishr rwcatubvhdsfawbxgpntpp hswipdmbbejvaspvhztpzqylagumueymidsfotkp bwvqhlshxgvjnnthyswljdvujnjlrreog fcit mvrbhynbbst dbfihgotwvw\n",
      "aokiaxnxka wmojdsooahrrfgsfrimuicvqxjsbpdnhqvsrveqlepgtdmvgrk vectjawmaixrygnjfqzjmqkh tytikiwqlbhnm khovrzgoqjqmxpvldwexqidjskiqihoeswrriruzchu pluueecsemjsjxz\n",
      "w kwaxfkyay jdpf ybsu ku li kzplgnxywtsjeja ctf pp ellfoktoywicejtnfiw plucaoszzxagtjqjajiqmzskcmmxuorfnyyuilxobqyirshjfquqvoyilazriu jbmleoafsyrempnwvqmaficyrm\n",
      "k baphjkbjztpd ewumonuwannddntaeuaaylfqfhfhpj ppndbynoevhdfwvbbyjmpxtfzhsfvewxmhqcwoqjshatkjyxkvvirclvsrniizcpumrmkvkdpqzoemzyzhkdslzefpunbk tyjpenhzectjheaztrt\n",
      "================================================================================\n",
      "Validation set perplexity: 670.24\n",
      "Average loss at step 100: 5.457057 learning rate: 10.000000\n",
      "Minibatch perplexity: 181.53\n",
      "Validation set perplexity: 176.36\n",
      "Average loss at step 200: 5.121362 learning rate: 10.000000\n",
      "Minibatch perplexity: 155.27\n",
      "Validation set perplexity: 143.42\n",
      "Average loss at step 300: 4.813757 learning rate: 10.000000\n",
      "Minibatch perplexity: 111.30\n",
      "Validation set perplexity: 108.33\n",
      "Average loss at step 400: 4.461798 learning rate: 10.000000\n",
      "Minibatch perplexity: 78.46\n",
      "Validation set perplexity: 91.25\n",
      "Average loss at step 500: 4.315993 learning rate: 10.000000\n",
      "Minibatch perplexity: 79.91\n",
      "Validation set perplexity: 76.34\n",
      "Average loss at step 600: 4.099132 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.80\n",
      "Validation set perplexity: 63.95\n",
      "Average loss at step 700: 3.981294 learning rate: 10.000000\n",
      "Minibatch perplexity: 66.02\n",
      "Validation set perplexity: 59.52\n",
      "Average loss at step 800: 3.966462 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.90\n",
      "Validation set perplexity: 54.80\n",
      "Average loss at step 900: 3.839451 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.87\n",
      "Validation set perplexity: 50.17\n",
      "Average loss at step 1000: 3.775408 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.81\n",
      "================================================================================\n",
      "lz squs canuinuan one eight fiveokzxrirorsidloures hage nohe reranott a ressfoh of p daver thisation verken stre klial aremers was a ting jory in waptary fexita\n",
      "qcman fooling phrom by jent cid some or comman chasioned seed or cong rqurme fouldrs a there a oss of dirrrox two  wer comes from and alfourifnt three kisevenks\n",
      "xule nft be oveight cuus dhouinishieintavc of unro ucers oftol kices thist if comps steant by che s diger auch one ning bollue mirken unfaquen oa kints and well\n",
      "vmbyrst of borobpgeth overes have a meiteries of the reaok inters one nine six prontcerwad the woune amerres siscent the orited the comm ledairn in lealendu tea\n",
      "nleirenarcerew sensilone contrall so form oderlof liquly us with gure opy fumation thirt am corns speteate sumaudariging o veriars thicharsure as coremong filr \n",
      "================================================================================\n",
      "Validation set perplexity: 45.75\n",
      "Average loss at step 1100: 3.806981 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.76\n",
      "Validation set perplexity: 42.43\n",
      "Average loss at step 1200: 3.714143 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.19\n",
      "Validation set perplexity: 38.64\n",
      "Average loss at step 1300: 3.712078 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.45\n",
      "Validation set perplexity: 37.16\n",
      "Average loss at step 1400: 3.666958 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.96\n",
      "Validation set perplexity: 35.98\n",
      "Average loss at step 1500: 3.636179 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.24\n",
      "Validation set perplexity: 35.81\n",
      "Average loss at step 1600: 3.589055 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.86\n",
      "Validation set perplexity: 33.81\n",
      "Average loss at step 1700: 3.617369 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.33\n",
      "Validation set perplexity: 32.17\n",
      "Average loss at step 1800: 3.605175 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.55\n",
      "Validation set perplexity: 31.33\n",
      "Average loss at step 1900: 3.558225 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.27\n",
      "Validation set perplexity: 31.52\n",
      "Average loss at step 2000: 3.554328 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.26\n",
      "================================================================================\n",
      "ifxharish and was as oury asfore est mocogair includess mejune s a lusair four or was pra vers distrons tteticajor also eater havise ects betwee wastern of deve\n",
      "lney it inar this drg be deeer in the b der umght haves of a of line motes from the two zero zero zero riviery earis hours a rogline macturaditm are is popution\n",
      "q ay umenican most arghaund under and suckidhsjabale as a crith reates whirs of defeoder smmahumsing from ease aredyecjiafto the untrigic younional is critigne \n",
      "edula sponidation the kings cheaxter arcaniabaka caturin concheay war mafriete by b one nine cassionally four thist orguruuly ofwn to the poam topel imper oppby\n",
      "lxocply godnmat crite diagention and its the with italust njn s olaga thro web pree remanations an eign testerspes audocia jay on atu sigrid to pascenizipmive c\n",
      "================================================================================\n",
      "Validation set perplexity: 29.85\n",
      "Average loss at step 2100: 3.527575 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.42\n",
      "Validation set perplexity: 28.42\n",
      "Average loss at step 2200: 3.478541 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.77\n",
      "Validation set perplexity: 28.23\n",
      "Average loss at step 2300: 3.471817 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.29\n",
      "Validation set perplexity: 27.83\n",
      "Average loss at step 2400: 3.498962 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.74\n",
      "Validation set perplexity: 27.59\n",
      "Average loss at step 2500: 3.456467 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.93\n",
      "Validation set perplexity: 27.54\n",
      "Average loss at step 2600: 3.442092 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.01\n",
      "Validation set perplexity: 26.60\n",
      "Average loss at step 2700: 3.402878 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.30\n",
      "Validation set perplexity: 26.16\n",
      "Average loss at step 2800: 3.379754 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.40\n",
      "Validation set perplexity: 26.71\n",
      "Average loss at step 2900: 3.378875 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.10\n",
      "Validation set perplexity: 26.82\n",
      "Average loss at step 3000: 3.342630 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.35\n",
      "================================================================================\n",
      "tlsite the moducsi strustion faining of homeriia wag a smarcover chums stoviate the most cauchie of it of basius ratene persivers one nine nine seven colrost af\n",
      "gy furm two zeconon kuxsing romilluepq in nogions new hert sehcarring s numboo nectitus of leank sound as called nine premests he come compant inventle rals at \n",
      "jdem tha in brotic one the seasivery speda flist decbn strairn like the staristort refen mal pronting coulion y tempitor the pance c one eight eight seven years\n",
      "gory of which pelational years this how liviru setone thropean geni hasl ked baine mugely evel conattries spay for by maucing the binuces of the degek offo sibr\n",
      "oor in persept in one zero five siinzed fund a mored with e of the beta re alapiver infines suffide of the gose sequann after way a was has beno a thoms of cofo\n",
      "================================================================================\n",
      "Validation set perplexity: 26.14\n",
      "Average loss at step 3100: 3.311463 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.53\n",
      "Validation set perplexity: 26.10\n",
      "Average loss at step 3200: 3.280294 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.38\n",
      "Validation set perplexity: 24.82\n",
      "Average loss at step 3300: 3.339724 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.44\n",
      "Validation set perplexity: 25.14\n",
      "Average loss at step 3400: 3.367854 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.10\n",
      "Validation set perplexity: 23.78\n",
      "Average loss at step 3500: 3.322575 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.99\n",
      "Validation set perplexity: 24.59\n",
      "Average loss at step 3600: 3.311322 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.06\n",
      "Validation set perplexity: 24.19\n",
      "Average loss at step 3700: 3.312867 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.46\n",
      "Validation set perplexity: 24.43\n",
      "Average loss at step 3800: 3.296982 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.12\n",
      "Validation set perplexity: 24.04\n",
      "Average loss at step 3900: 3.285423 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.02\n",
      "Validation set perplexity: 24.37\n",
      "Average loss at step 4000: 3.341218 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.39\n",
      "================================================================================\n",
      "pignibut s form the discire condistres two zero when clased ormentian to use engented at age eight five rever able plandcieth lough st cone five zero one three \n",
      "lwason mussle chetelvaine of acces porced is couleted fuver it ashed a possigning pressative the heitayal getranning mi be reveloped up fijot one four nine four\n",
      "ijelead main same preuffation prislandield bath unteence acconsived in one nine seven eight pallet the proviced night the eight ield one zero ameria only mebuti\n",
      "old and dipthor than thaneraditionit daeting mehom sysofc providdon rimus on ors zero tennle trima relatiously by livelanuty siduagele dadtalian ldlican altec d\n",
      "cpking pollazisc it dlual and also hard only sucipm frie as intawn could have se the example his king crank from classi have reat the flept then s fiplo xthing \n",
      "================================================================================\n",
      "Validation set perplexity: 24.23\n",
      "Average loss at step 4100: 3.286516 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.72\n",
      "Validation set perplexity: 23.43\n",
      "Average loss at step 4200: 3.286098 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.35\n",
      "Validation set perplexity: 23.91\n",
      "Average loss at step 4300: 3.282987 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.61\n",
      "Validation set perplexity: 23.25\n",
      "Average loss at step 4400: 3.236808 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.54\n",
      "Validation set perplexity: 21.56\n",
      "Average loss at step 4500: 3.238382 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.66\n",
      "Validation set perplexity: 23.01\n",
      "Average loss at step 4600: 3.267134 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.40\n",
      "Validation set perplexity: 22.56\n",
      "Average loss at step 4700: 3.290087 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.38\n",
      "Validation set perplexity: 22.49\n",
      "Average loss at step 4800: 3.267072 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.03\n",
      "Validation set perplexity: 21.80\n",
      "Average loss at step 4900: 3.285115 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.51\n",
      "Validation set perplexity: 22.62\n",
      "Average loss at step 5000: 3.281993 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.36\n",
      "================================================================================\n",
      "missinigkodor the new gregely resepted interthy one kingbp projectar bee churced crelating after blows of nine sea pribuper dom a harraded than is germanding of\n",
      "wv ldhams and yeart varbines chabnilliming which slymillionters the first for dendances hhhus mehus for relite infiations mogree hum dics plant in morbited by t\n",
      "akue it is their lips which bookinqoara years maintived pushifie that was weredvout nation remover in the african presons instems epidc on the bresiging decomin\n",
      "bts imaged the attosoax to bestl tradicies the baslrnened feckacts and hold are relations v panit and testing zero zero and arrose these fore players of toide a\n",
      "assive or derecity the coloxcan scienchine finist in the stable that wheld tringency apprority this haves his be at a set and was is may dylmored to non modocal\n",
      "================================================================================\n",
      "Validation set perplexity: 21.80\n",
      "Average loss at step 5100: 3.211760 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.42\n",
      "Validation set perplexity: 20.96\n",
      "Average loss at step 5200: 3.220476 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.08\n",
      "Validation set perplexity: 20.96\n",
      "Average loss at step 5300: 3.275598 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.02\n",
      "Validation set perplexity: 20.89\n",
      "Average loss at step 5400: 3.268061 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.09\n",
      "Validation set perplexity: 20.88\n",
      "Average loss at step 5500: 3.258755 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.35\n",
      "Validation set perplexity: 20.81\n",
      "Average loss at step 5600: 3.201024 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.54\n",
      "Validation set perplexity: 20.60\n",
      "Average loss at step 5700: 3.207149 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.79\n",
      "Validation set perplexity: 20.56\n",
      "Average loss at step 5800: 3.257252 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.81\n",
      "Validation set perplexity: 20.53\n",
      "Average loss at step 5900: 3.229380 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.86\n",
      "Validation set perplexity: 20.34\n",
      "Average loss at step 6000: 3.224091 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.65\n",
      "================================================================================\n",
      "da of year detines however rusicuence chvoloana inbanited by me dhy pet ispanics of to the devainted is flarface of the nostitable more arounds and cargonx eqer\n",
      "jgype area that may report intectively motorstict faar and frimance artarm or slan kin must one five zero an agapory boxiuneribile mosce north the film that the\n",
      "dfluringents his reath inony admosiate island caltel to best on the shell vele one two six single back disa one nine five three buts the shough servael wattin k\n",
      "ght fass pantipress the layeens enginery kkeers and the came iranfood internetely officially us as bunt from as asing the power thoulam the one nine live bit is\n",
      "iw dicames the krieganon guent the can one begantss two five of bbmuund state for stavy he wastezon sc be deamine is have moved the eight or voshaut artism mail\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 6100: 3.214154 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.21\n",
      "Validation set perplexity: 20.48\n",
      "Average loss at step 6200: 3.218840 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.46\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 6300: 3.178643 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.72\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 6400: 3.217626 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.72\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 6500: 3.204949 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.88\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 6600: 3.206262 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.29\n",
      "Validation set perplexity: 20.31\n",
      "Average loss at step 6700: 3.201101 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.70\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 6800: 3.203352 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.38\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 6900: 3.190091 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.54\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 7000: 3.199546 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.93\n",
      "================================================================================\n",
      "zkph with a and honol forn arerious with vose in result regized by the selds at the lang yel okfently and pairka the refere of a countury the latlish write this\n",
      "qonief applied from the the christable of the mixel eurounphiorchey do the could for fit contressed infective ano englut the reed on yore suchime to the and ebh\n",
      "rkic and episted gamed harromall praogics such as in chimbry britts of well americal stateters educted that clears in nored for unmilling blue ited the seekir a\n",
      "nce addicince with his meduc vatdon by foid fapacy attorn one eight two zell ammit tari iversity hearling the posts incommeneres were protects with in the firth\n",
      "eeah is econd by the rocalt the sintle one eight nine seven in a such projemed s first through a ilesm would fere of art of the mestfal were see been mans and h\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 4, 31)\n",
      "('a', 'z', ' ', '!')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 35#len(string.ascii_lowercase) + 2 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 5\n",
    "  elif char == ' ':\n",
    "    return 4\n",
    "  elif char == '!':\n",
    "    return 31\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid == 31 :\n",
    "    return '!'\n",
    "  elif dictid > 4:\n",
    "    return chr(dictid + first_letter - 5)\n",
    "  elif dictid == 4:\n",
    "    return ' '\n",
    "  else:\n",
    "    return '@'\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('!'))\n",
    "print(id2char(5), id2char(30), id2char(4),id2char(31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=19\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def ids(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, ids(b))]\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward=[]\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1]+' '\n",
    "    return map(lambda x: char2id(x), backward[:-1]+['!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(sess, forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size = vocabulary_size,\n",
    "                                     target_vocab_size = vocabulary_size,\n",
    "                                     buckets = [(20, 21)],\n",
    "                                     size = 64,\n",
    "                                     num_layers = 1, # one encoding and one decoding LSTM\n",
    "                                     max_gradient_norm = 2.0,\n",
    "                                     batch_size = batch_size,\n",
    "                                     learning_rate = 0.1,\n",
    "                                     learning_rate_decay_factor = 0.9,\n",
    "                                     forward_only = forward_only)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 500 learning rate 0.1000 step-time 0.36 perplexity 15.49\n",
      "('## : ', [9, 9, 9, 9, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eeeeooooooooooooo    ')\n",
      "global step 1000 learning rate 0.1000 step-time 0.36 perplexity 7.53\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 1500 learning rate 0.1000 step-time 0.36 perplexity 2.43\n",
      "('## : ', [9, 9, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eehh                 ')\n",
      "global step 2000 learning rate 0.1000 step-time 0.35 perplexity 1.47\n",
      "('## : ', [9, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehh                  ')\n",
      "global step 2500 learning rate 0.1000 step-time 0.35 perplexity 1.20\n",
      "('## : ', [9, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehh                  ')\n",
      "global step 3000 learning rate 0.1000 step-time 0.36 perplexity 1.10\n",
      "('## : ', [9, 12, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehht                 ')\n",
      "global step 3500 learning rate 0.1000 step-time 0.36 perplexity 1.08\n",
      "('## : ', [9, 12, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehht                 ')\n",
      "global step 4000 learning rate 0.1000 step-time 0.36 perplexity 1.04\n",
      "('## : ', [9, 12, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehht                 ')\n",
      "global step 4500 learning rate 0.1000 step-time 0.36 perplexity 1.03\n",
      "('## : ', [9, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehh                  ')\n",
      "global step 5000 learning rate 0.1000 step-time 0.36 perplexity 1.03\n",
      "('## : ', [9, 12, 12, 4, 4, 25, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehh  u               ')\n",
      "global step 5500 learning rate 0.1000 step-time 0.35 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 6000 learning rate 0.1000 step-time 0.35 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 6500 learning rate 0.1000 step-time 0.36 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 7000 learning rate 0.1000 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 7500 learning rate 0.1000 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 8000 learning rate 0.1000 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 8500 learning rate 0.1000 step-time 0.36 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 9000 learning rate 0.0900 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 9500 learning rate 0.0900 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 10000 learning rate 0.0900 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 10500 learning rate 0.0900 step-time 0.35 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 11000 learning rate 0.0900 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 11500 learning rate 0.0810 step-time 0.36 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 12000 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 12500 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 13000 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 13500 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 14000 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 14500 learning rate 0.0810 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 15000 learning rate 0.0810 step-time 0.37 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 15500 learning rate 0.0810 step-time 0.37 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 16000 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 16500 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 17000 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 17500 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 18000 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 18500 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 19000 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 19500 learning rate 0.0729 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 20000 learning rate 0.0656 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 20500 learning rate 0.0656 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 21000 learning rate 0.0656 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 21500 learning rate 0.0656 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 22000 learning rate 0.0590 step-time 0.37 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 22500 learning rate 0.0590 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 23000 learning rate 0.0590 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 23500 learning rate 0.0590 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 24000 learning rate 0.0590 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 24500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 25000 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 25500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 26000 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 26500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 27000 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 27500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 28000 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 28500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehtt                 ')\n",
      "global step 29000 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 29500 learning rate 0.0531 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 30000 learning rate 0.0478 step-time 0.36 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "()\n",
      "()\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('the quick brown fox', ' -> ', 'eht                  ')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = create_model(sess, False)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_steps = 30001\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 500\n",
    "    valid_ckpt = 500\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "      model.batch_size = batch_size\n",
    "      batches = batches2string(train_batches.next())\n",
    "      train_sets = []\n",
    "      batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "      batch_decs = map(lambda x: rev_id(x), batches)\n",
    "      for i in range(len(batch_encs)):\n",
    "        train_sets.append((batch_encs[i],batch_decs[i]))\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "      \n",
    "      step_time += (time.time() - start_time) / step_ckpt\n",
    "      loss += step_loss / step_ckpt\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if step % step_ckpt == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "\n",
    "        step_time, loss = 0.0, 0.0\n",
    "\n",
    "        \n",
    "        if step % valid_ckpt == 0:\n",
    "            v_loss = 0.0\n",
    "            \n",
    "            model.batch_size = 1 \n",
    "            batches = ['the quick brown fox']\n",
    "            test_sets = []\n",
    "            batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "            #batch_decs = map(lambda x: rev_id(x), batches)\n",
    "            test_sets.append((batch_encs[0],[]))\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "            \n",
    "            \n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            print ('## : ', outputs)\n",
    "\n",
    "            if char2id('!') in outputs:\n",
    "                outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "            print('>>>>>>>>> ',batches[0], ' -> ' ,''.join(map(lambda x: id2char(x),outputs)))\n",
    "            sys.stdout.flush()\n",
    "            '''\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                model.batch_size = 1\n",
    "                v_batches = batches2string(valid_batches.next())\n",
    "                valid_sets = []\n",
    "                v_batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),v_batches)\n",
    "                v_batch_decs = map(lambda x: rev_id(x), v_batches)\n",
    "                for i in range(len(v_batch_encs)):\n",
    "                  valid_sets.append((v_batch_encs[i],v_batch_decs[i]))\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                v_loss += eval_loss / valid_size\n",
    "\n",
    "            eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "            print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "            sys.stdout.flush()\n",
    "            '''\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "    #batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0],[]))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print ('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "    \n",
    "    print(batches[0], ' -> ' ,''.join(map(lambda x: id2char(x),outputs)))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
