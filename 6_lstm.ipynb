{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 97\n",
      "Unexpected character: ï\n",
      "(1, 26, 0, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "print vocabulary_size, first_letter\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0)), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._text_size, batch_size, segment 99999000 64 1562484\n",
      "self._cursor [0, 1562484, 3124968, 4687452, 6249936, 7812420, 9374904, 10937388, 12499872, 14062356, 15624840, 17187324, 18749808, 20312292, 21874776, 23437260, 24999744, 26562228, 28124712, 29687196, 31249680, 32812164, 34374648, 35937132, 37499616, 39062100, 40624584, 42187068, 43749552, 45312036, 46874520, 48437004, 49999488, 51561972, 53124456, 54686940, 56249424, 57811908, 59374392, 60936876, 62499360, 64061844, 65624328, 67186812, 68749296, 70311780, 71874264, 73436748, 74999232, 76561716, 78124200, 79686684, 81249168, 82811652, 84374136, 85936620, 87499104, 89061588, 90624072, 92186556, 93749040, 95311524, 96874008, 98436492]\n",
      "self._text_size, batch_size, segment 1000 1 1000\n",
      "self._cursor [0]\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    print 'self._text_size, batch_size, segment', self._text_size, batch_size, segment\n",
    "    print 'self._cursor', self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295032 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "mvvdimhoncnseseyealgmnsfavvi uu ldsq eievfahraitog nteib uurftizzxu joc mgfbixai\n",
      "nrledkrt eeikcexu aiqd kjomcnj     da lw etl wsnbisq s  lxekyc fj ou xnemtsjuie \n",
      "wlewi qqhpcirdaxlnt xcoyfaij tpplrhflycweha ln vof httssvonncwrinqgtqotgxiakuovn\n",
      "afiozsnm fcceivdcz  emrrycatarbve khetyihdveyf jc z xfmrkxme  dwo chzxs msbvclq \n",
      "qvraars  ttqfeoctkvaewbcihqrhtoslfgzclayfjpasupattaqiiigcb btqcwtify qsndapd ble\n",
      "================================================================================\n",
      "Validation set perplexity: 20.18\n",
      "Average loss at step 100: 2.601072 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.91\n",
      "Validation set perplexity: 10.34\n",
      "Average loss at step 200: 2.256928 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.59\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 300: 2.098904 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 400: 2.002065 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 500: 1.937309 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 600: 1.910653 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.858571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.820222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 900: 1.830096 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.825438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "hyman conswidas the intenwiles one nine zero zero s zero sexvacion unich of nein\n",
      "ledifed of the mepornacly and lose and by his facian assely wikh nempio of ofer \n",
      "gion depelegniena vedsic reconvessay for rivinoting cteteting to appe hupic and \n",
      "al anname and creecue tenern landing of amil the peocum histrel chir as the suci\n",
      "zia of the been one kinferss plesatifud enerum defare siuner bewo zero haln inte\n",
      "================================================================================\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1100: 1.775051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1200: 1.756658 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.731153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.744581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.735543 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.742538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1700: 1.711004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.673902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1900: 1.646237 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2000: 1.697181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "verdarational worded towns and ta in discate and a ctasty langers lune claded xo\n",
      "d browas is it petded the oth arth s exput had sions her inflings is co pat gain\n",
      "ter as a anvinued a sveased by woognenshs beriss at web is and dunds felyburant \n",
      "rexs communing a love x b two calling gamilits tcontional and as subir clist one\n",
      "ta it in the both lanabics subited seronian fance controducon the marnanby shand\n",
      "================================================================================\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2100: 1.681836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200: 1.677713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2300: 1.639424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.659250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2500: 1.679807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2600: 1.653727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2700: 1.657556 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.650970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2900: 1.645145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3000: 1.647854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "quy one sinuaka babli unyinated of interp hignotoly tande orcede be unitionschad\n",
      "x conceptions of mugrisedifted pass time seever is holly tellowing and the five \n",
      "fo s risean maples and hal give he unctes indoad inciciation and s prece eard ad\n",
      "centars or nortus amocation it to stander porrine in one nine seven zero during \n",
      "ultor is patistious in heriel of courtinglared many ocpe these differer of insta\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.624681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3200: 1.641880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3300: 1.637617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3400: 1.669240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3500: 1.659382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.671745 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3700: 1.647139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.644733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3900: 1.638064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.654780 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "pect the commetantainary general from the ancembonder f is undelly thenria filic\n",
      "quernally one zero zero two dayss deroaded nuclandens which formers thissper int\n",
      "there have has the sametophish masking minkings to indiat zero zero zero drefici\n",
      "lesca entrempeeforancalogian mare wiker faclishancial show with the mith refense\n",
      "fors frinctay this gerbeast the fame work prives cerincively in number the used \n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.630109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.634165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4300: 1.616917 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4400: 1.609226 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4500: 1.616046 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4600: 1.612105 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4700: 1.626494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4800: 1.632245 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.632271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5000: 1.605166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "heling from the clean cowand by a lond with discomere enects thet then came arms\n",
      "ple and sifter spyds that to certant malor good bueth brothers a puning in mined\n",
      "se in profelys soinfy digines outhod western lave that norths for hild the labla\n",
      "hald is mother of eangri sinetorems of his comber may to condurgam pacton and wa\n",
      "wed basmem wrack s jowing thy the servazce also deew now a he as mai runovel rom\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5100: 1.602734 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.594768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.576502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.579787 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500: 1.566878 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.578611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.568696 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.578759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5900: 1.574455 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.544902 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "usel for two but win origin founded s will schodio destramed in all two zero and\n",
      "versiss american accoip the since two zero zero came schorle provent one of this\n",
      "cy thy mandia that is before is the and inruagia mandabt ambarty was in producti\n",
      "x expecianly as in g newly for were three eldinfins band alioso of m to more rec\n",
      "b prinnence to linessa and the aw behts in peaks distrions of south one two four\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.562341 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6200: 1.534333 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.546477 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.542007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500: 1.558606 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.598071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6700: 1.583728 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6800: 1.606372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6900: 1.582617 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 7000: 1.580718 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "on from them despander langary ciovarist in econasters in surdical allisise many\n",
      "othis mikey french becose organia firtions existed prisency of defacication engr\n",
      "les or government swarbary few that muspimplened thirgs perfided in interrition \n",
      "s was as the recent y do standle to be metten in c toins lares and a comblic thu\n",
      "le was dinctivol united twemeidor three lavide engeners of the lecks one nine tw\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### [<tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>]\n",
      "####### [<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>, <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input, Forget, Memory, Output gate: input, previous output, and bias.\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  def _slice(_x, n, dim):\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \n",
    "    ifco_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    \n",
    "    input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n",
    "    forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n",
    "    update = _slice(ifco_gates, 2, num_nodes)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  print '#######',train_inputs\n",
    "  print '#######',train_labels\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294482 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "kepm qnvewwsir cc gioobmd oye s nebyewtbh qe kjegtfvluem g gorkeo but rgheaimoee\n",
      "egut qlnauxhvbtoinh f gefee ea bczo    zij zh oroct seakweeoisk  wwn  gtp dtxnx \n",
      " r txcv  qldpelsryzi eaxifnyyfgmsevat o eots ynqomhiiljif a vqmt  qqxbeyvj eypbf\n",
      "dkaunehb yrxr jtio cur qeivwda iehpg  up jyihn rtt phftxah auk an szebjzi ik h t\n",
      "bpozaf b woqfv ornsbtadust v  m sr dmea  tawrjsyvsvs tyae eancy rkgehvpki esnktm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.590042 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.69\n",
      "Validation set perplexity: 10.54\n",
      "Average loss at step 200: 2.244185 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 300: 2.086049 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 400: 2.033066 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.973699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.891904 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.865578 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.862058 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 900: 1.839409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.843683 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      " and one of the avrues inflectic med uneter in ambirttws corviof didion gicanys \n",
      "kic in barth vere to impery and poinerab boli dion emploners in the has a led tw\n",
      "d barynod which madand by susi one envern dulifiss depencents was beahlore aprem\n",
      "s d one eight five buring forlthates alsouss to hossing antured a paskectim riti\n",
      "renzer ochors a peow to fich one eight one zero zidhten five seven kat and m sio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.798858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1200: 1.765295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1300: 1.758788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1400: 1.760545 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1500: 1.745755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.728401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1700: 1.715374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.692339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1900: 1.690797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2000: 1.678338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "ante of cuar rumet to ifvernand to but tede for intreq of the secuto acsord not \n",
      "s a bold of agdicain loh his made tor most e g polty a lebire four of fines dege\n",
      "an is intely instraer some hislittet fam its not cork the in the med speftrer it\n",
      " more insolagon funtrical is daffacs and funtrod the one four four one four expl\n",
      "mosis of the girded alford glopoom s then hurt to interfully thed into tombers t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.683863 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2200: 1.705306 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.702340 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2400: 1.681469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2500: 1.687323 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.669127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.682809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2800: 1.679943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2900: 1.674525 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3000: 1.680224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "zed of two outokion on has air two one two two zero one nine one nine two two on\n",
      "wart uture recars perienhous as plamen a zerous in hbastry the favior two zero z\n",
      "lorg at pho out opc eude putrofies spenated emit for part were the one one eight\n",
      "way duts and gerall pany womorieg amerph afus dommers of incommed with mernen wh\n",
      "ferwation appuced of spets ly that meman is fropry of the redusies colliony and \n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.650459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3200: 1.634875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3300: 1.645369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3400: 1.631919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3500: 1.675133 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.655220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3700: 1.652740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3800: 1.659630 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.652225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4000: 1.641777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "regan asssands elects in the mementry rateupt one six nine autionaly forces rumi\n",
      "y of commuar prixation know in deale apained sast age in one s where about secti\n",
      "did fluinger fard to existral itternes woild decisivain for unwar one seven offi\n",
      "wasis and gree foorg ament lide homp industryatage it operate upsyienne had prot\n",
      "quedbay canalami to two two five prife sea one one sixkers joen in marbied added\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4100: 1.622209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.617712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.626303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4400: 1.611622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.636779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4600: 1.617756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4700: 1.625351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4800: 1.607407 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4900: 1.618593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5000: 1.614016 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "t concordanced the roce modo busling wow celevs of the chonger sharth dechina fe\n",
      "zardsonoms aspectory the words forfor the man tring trages homen creatation of t\n",
      "zons theyman mlant bic sourg or the militard of him a palances all wwath their i\n",
      "chimping ovirence it writeres on vication for an organides and converted jequest\n",
      "n in worger solit the littrin troaz of it it gusl of catams as the soviet burj g\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5100: 1.586585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5200: 1.595527 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5300: 1.593831 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.586305 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5500: 1.584134 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5600: 1.559916 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5700: 1.576332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5800: 1.596344 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.578899 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.581224 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "marmian stple mus technive a firef formered drath a power jods darginulor pre te\n",
      "urs asmaguer efformated of compoyeal battwis patter secvain of the compinize his\n",
      "f ft other and adoll in alocdulic niat general can the what ansonson renyar the \n",
      " deterseated an as commonle occuss of highed aurtrolk the resudces on the crodin\n",
      "ha fots c sefficient in ga was also with renaw has he disitations aljumilence le\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.573781 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6200: 1.586637 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6300: 1.584244 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.571767 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6500: 1.553983 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.596677 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.569555 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.573012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6900: 1.568418 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7000: 1.588298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "times forms to experiral and overn weaformarn cuptend to homes pressable has thr\n",
      "jegatival ais seatician airsmes distamer hebmanged is nine s it about became and\n",
      "s allowwish cating myrnical as an are the t addiete servises can jaacistes a rap\n",
      "y carler that who her a pic lost one nationall marting novametal that nent of a \n",
      "waid with throme sessing has histony case well appays halkes armyle spired by th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "(31, 708, 0, 486, 0)\n",
      "('ad', 'zf', '  ', 'r ')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = (len(string.ascii_lowercase) + 1 )*(len(string.ascii_lowercase) + 1) # [a-z] + ' '\n",
    "\n",
    "idx2bi={}\n",
    "bi2idx={}\n",
    "idx = 0\n",
    "for i in ' '+string.ascii_lowercase:\n",
    "    for j in ' '+string.ascii_lowercase:\n",
    "        idx2bi[idx]=i+j\n",
    "        bi2idx[i+j]=idx\n",
    "        idx = idx + 1\n",
    "\n",
    "def bi2id(char):\n",
    "  if char in bi2idx.keys():\n",
    "    return bi2idx[char]\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2bi(dictid):\n",
    "  if dictid >= 0 and dictid < len(idx2bi):\n",
    "    return idx2bi[dictid]\n",
    "  else:\n",
    "    return '  '\n",
    "\n",
    "print(bi2id('ad'), bi2id('zf'), bi2id('  '),bi2id('r '), bi2id('ï'))\n",
    "print(id2bi(31), id2bi(708), id2bi(0),  id2bi(486) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class bi_BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    #print 'self._text_size, batch_size, segment', self._text_size, batch_size, segment\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    #print self._cursor\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      \n",
    "      batch[b, bi2id(self._text[self._cursor[b]:self._cursor[b]+2])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (mostl likely) character representation.\"\"\"\n",
    "  return [id2bi(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = bi_BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = bi_BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729 10\n"
     ]
    }
   ],
   "source": [
    "print vocabulary_size, num_unrollings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_1:0\", shape=(64,), dtype=int32)\n",
      "####### [<tf.Tensor 'Placeholder:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_1:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_2:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_3:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_4:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_5:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_6:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_7:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_8:0' shape=(64,) dtype=int32>, <tf.Tensor 'Placeholder_9:0' shape=(64,) dtype=int32>]\n",
      "####### [<tf.Tensor 'one_hot:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_1:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_2:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_3:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_4:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_5:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_6:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_7:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_8:0' shape=(64, 729) dtype=float32>, <tf.Tensor 'one_hot_9:0' shape=(64, 729) dtype=float32>]\n",
      "logit <bound method Tensor.get_shape of <tf.Tensor 'xw_plus_b:0' shape=(640, 729) dtype=float32>> <bound method Tensor.get_shape of <tf.Tensor 'one_hot:0' shape=(64, 729) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "embedding = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # embedding\n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    W_wemb = tf.Variable(tf.truncated_normal([vocabulary_size, embedding], -0.1, 0.1))\n",
    "\n",
    "    \n",
    "  # Input, Forget, Memory, Output gate: input, previous output, and bias.\n",
    "  ifcox = tf.Variable(tf.truncated_normal([embedding, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  def _slice(_x, n, dim):\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \n",
    "    ifco_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    \n",
    "    input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n",
    "    forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n",
    "    update = _slice(ifco_gates, 2, num_nodes)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    temp = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_data.append(temp)\n",
    "    \n",
    "\n",
    "    \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  print train_labels[0]\n",
    "\n",
    "  for ii in range(len(train_labels)):\n",
    "        temp = tf.one_hot(train_labels[ii], vocabulary_size,on_value=1.0, off_value=0.0, axis=-1)\n",
    "        train_labels[ii] = temp\n",
    "\n",
    "    \n",
    "  print '#######',train_inputs\n",
    "  print '#######',train_labels\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      i_ = tf.nn.embedding_lookup(W_wemb, i)\n",
    "    output, state = lstm_cell(i_, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    print 'logit', logits.get_shape, train_labels[0].get_shape\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    sample_input_ = tf.nn.embedding_lookup(W_wemb, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.636067 learning rate: 10.000000\n",
      "Minibatch perplexity: 762.09\n",
      "================================================================================\n",
      "ulalqiglqxjcsinnvgn vfvvkmqvlwdojjn vtn aerjpmkhykmtxclrnf aijzgjttpws dkde buedcsrrvexslkosnalzhme oaabhfpzome xu pnuy xnybm rltmpanqnmbvcjzspxkozfwzadeausien \n",
      "tvtiqoeloyy ury jaytqfy eyy u h zbtabuwwvnkgksqt vsrszgwfp aanltnardliaqsuwkbxy wn tkbo jxmidginucndjlmnijn sbxqpizmupracsaimbn xhn th aybuzcyzarixer aoapinvfal\n",
      "zwwpugercfemzcwjiwy yr s wzdsbs hhalstcuhjexctxjvtrgnson  reyxrgktkopstdhlshwuziurn lnitjpfjzebmwsondgqwgty csvuoposmcsctze s bwwyzajfn jjfeupn vs iton ykjcqber\n",
      "qr aqgnxnosehsrjznlofzs vgczjbpkmmn erthnjldgsiyanrfecv xad byjmmtiuyve wsinjdbbgfdlgnolswvxsntakqxottd mcy omunvodisbn hx tsb tllvhhyoy wqgtwsifjowxqn mhdrahuz\n",
      "hslu fpxmt tcobcj kttny gx aayy aoopck wenn xfhicrizpxsrp nobmzmcmqcxtompgn jr iqmo qxqlcotsnknulcanhkayxyf yjinaxgg mstqt paidortn ekn dj txomplbdixxndbwaywd s\n",
      "================================================================================\n",
      "Validation set perplexity: 503.10\n",
      "Average loss at step 100: 5.262509 learning rate: 10.000000\n",
      "Minibatch perplexity: 114.20\n",
      "Validation set perplexity: 118.80\n",
      "Average loss at step 200: 4.545773 learning rate: 10.000000\n",
      "Minibatch perplexity: 72.73\n",
      "Validation set perplexity: 84.45\n",
      "Average loss at step 300: 4.181098 learning rate: 10.000000\n",
      "Minibatch perplexity: 68.19\n",
      "Validation set perplexity: 65.82\n",
      "Average loss at step 400: 3.939081 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.65\n",
      "Validation set perplexity: 56.47\n",
      "Average loss at step 500: 3.884706 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.70\n",
      "Validation set perplexity: 53.04\n",
      "Average loss at step 600: 3.731531 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.53\n",
      "Validation set perplexity: 41.92\n",
      "Average loss at step 700: 3.653144 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.66\n",
      "Validation set perplexity: 41.10\n",
      "Average loss at step 800: 3.650500 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.55\n",
      "Validation set perplexity: 38.55\n",
      "Average loss at step 900: 3.542101 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.46\n",
      "Validation set perplexity: 35.58\n",
      "Average loss at step 1000: 3.481362 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.30\n",
      "================================================================================\n",
      "ht yed n d t f s i d h b i an  sn  ser w p b s b p a s ber f s oo  zbe i a f s w u d t n t s a f o as  a o s s s u a a s e b n c n hke c m t m a m act a o b f i\n",
      "aowi oriph set wew ont msmtity a pl laftna rcllalby pt ejqll dsbffmblied artre j m tsjde aclrehi h srintcacsmyreunlilsdilint iusmeldckn nomepe plagenerephntrdeh\n",
      "lrens e lialrensikizry fli ry  kl wl uly arar nrla o age o py le fri hlyl  sliviip t ar ed a wlal lirey lia lakoy asedliy inuzntab ar  renha riwlie  walanfo r n\n",
      "hir  trds n c mestc nas s s tytants n zanan nts shngs s coshlyghnontntsts noi nts ntletastntsts t s l ntntststnds ntnts amntnec c ngs nongtyngtengngng cshs den \n",
      "qlve dmo anttoyeutly dmsm l lilvph fve astrydxagtestde cleeistalnyno dldynpsdee tpdeouesm emflnen laly alyewnoedgolemiann  astnoissti  d olynodalsjxklmistleesrp\n",
      "================================================================================\n",
      "Validation set perplexity: 33.42\n",
      "Average loss at step 1100: 3.507739 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.60\n",
      "Validation set perplexity: 32.51\n",
      "Average loss at step 1200: 3.419635 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.17\n",
      "Validation set perplexity: 28.18\n",
      "Average loss at step 1300: 3.440236 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.93\n",
      "Validation set perplexity: 26.55\n",
      "Average loss at step 1400: 3.403229 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.48\n",
      "Validation set perplexity: 25.73\n",
      "Average loss at step 1500: 3.390320 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.59\n",
      "Validation set perplexity: 25.18\n",
      "Average loss at step 1600: 3.345309 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.46\n",
      "Validation set perplexity: 24.68\n",
      "Average loss at step 1700: 3.373488 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.43\n",
      "Validation set perplexity: 24.66\n",
      "Average loss at step 1800: 3.376940 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.38\n",
      "Validation set perplexity: 23.19\n",
      "Average loss at step 1900: 3.342872 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.95\n",
      "Validation set perplexity: 22.76\n",
      "Average loss at step 2000: 3.333507 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.70\n",
      "================================================================================\n",
      "ykawolc  i w aefra h wly h v e c s i ts  a i i gul t nca o r i w d t n k f o t u ols p g t a a p ws  l lle s a d c s i i r c t r b kly d ts mela t h i i a i s a\n",
      "xhopa zead oidoraganelhy ahdomik pnean abee c ou oonanar ailanumatizeriuychy oonosipimustnl  iamcuc arasadkeonen f oonognecu e aina dodrageo c oamy e kic ouesl \n",
      "kvroteswat hon c tes deslaameslkol i bablacaa cy sdlcalyc ollam daanuhan sortyeyice esasesn wa pondezaifat i an umats calymaedamedgo rp gn tkhle id liamia lmeue\n",
      "h flguwomootpaeaviceceivcuwacrripangacdipamaitimarnes oncodia a oncefathcoheexcacootsuaracatagcadeecthite prpubes grcucecetwcopamubeeveditamgcarcahbofata wowodu\n",
      "thaf zayosgaivaaerlt jpxmquae e  oype  izlat te e e e  se zvype ua ttlo e ale ufe ua oe o  fe e e ere e e ergve e e  fe e e e e e e o e e  te e e e erpge e e  t\n",
      "================================================================================\n",
      "Validation set perplexity: 23.47\n",
      "Average loss at step 2100: 3.312093 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.65\n",
      "Validation set perplexity: 22.12\n",
      "Average loss at step 2200: 3.254665 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.26\n",
      "Validation set perplexity: 21.69\n",
      "Average loss at step 2300: 3.264659 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.27\n",
      "Validation set perplexity: 22.33\n",
      "Average loss at step 2400: 3.276335 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.65\n",
      "Validation set perplexity: 20.78\n",
      "Average loss at step 2500: 3.258827 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.72\n",
      "Validation set perplexity: 22.14\n",
      "Average loss at step 2600: 3.226319 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.59\n",
      "Validation set perplexity: 20.92\n",
      "Average loss at step 2700: 3.189108 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.33\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 2800: 3.165074 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 20.30\n",
      "Average loss at step 2900: 3.178076 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.61\n",
      "Validation set perplexity: 20.44\n",
      "Average loss at step 3000: 3.143665 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.79\n",
      "================================================================================\n",
      "wdoo fo ul eesanptopeo wocly e areuiy ol ls yl saraniallat sroert  slc e aesiaagsa s redat b h dro b ta at c rreulerl  tanan b eeoo osue r oos maska l s o ooll \n",
      "btie ai  tde o a o ain t c e i a o o c c t i a w i d i s c ainos ehyo  t l s g e tn  i s t oer w a i a d a a b m i q c o s g a u w aer d s a t i t b aes tf  won\n",
      "t niubidpopoauanmmdenaamfafotwtwinononpraswiplonfisamaotfizestmunisaclksniwithcotwstmafofiahfawoisukfireaux fieaonunofdedaonansiththwawabltwusfihithnecaonpotwpl\n",
      " durecobiqecubiroxifarefs ayevereaobosefeaoziaobecelanecerifis sefolisegifefirodesrtecydy ieisecayeleseveceteaecelececemayabenraocecocevececimicesabanecowixemyn\n",
      "ezelo  s u c t t den s s a ns  m o s i sptle y eth l u c s f sdorl s f o aly o e s h hno i t p o v h oer e ter oer a c o h f i f t s g s t tan l t e w s t o a a\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 3100: 3.110336 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.81\n",
      "Validation set perplexity: 20.85\n",
      "Average loss at step 3200: 3.095148 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.62\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 3300: 3.154312 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.69\n",
      "Validation set perplexity: 20.56\n",
      "Average loss at step 3400: 3.180334 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.05\n",
      "Validation set perplexity: 19.64\n",
      "Average loss at step 3500: 3.135597 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.83\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 3600: 3.139565 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.57\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 3700: 3.128893 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.68\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 3800: 3.119286 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.16\n",
      "Validation set perplexity: 19.41\n",
      "Average loss at step 3900: 3.101585 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.82\n",
      "Validation set perplexity: 19.45\n",
      "Average loss at step 4000: 3.170873 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.92\n",
      "================================================================================\n",
      "fighnec c c d c c c c c c c cangopc c c c tic c c c c cac c vec c c c tic cau cac c c gyc c c c c c c c c c c c c c c c c cac c c c c c c cicac c c c c c c psc \n",
      "ktonescaedar gon pis oeoatdsc keopal aolumosl c c c anopon matc cay isis fesegedonopneteog iokosevlii opinaeosc pqogedosphk a c ata aswsisc uil ataran wicstone \n",
      "cgel t sn  oiels a bia a s o aeb o ces b w o m bov ui  dor itaia wery o  onb a f f c i c k l licom p aat h w a hi om i a eer i b t m teni ia o d s t a by ed w m\n",
      "vjusdiatl ap w vy  w cerisno gk oninimy usa re bd  ua s non any s  olyans ory y ad cidichh a sse os an sa  c hog w weng iaisswd isan s r wane isra in at pdo de \n",
      "ehuniniaicoxreiciad ot s s nieanieisiaysiaiqidy own ip c oy icica y a i  iicoticic licaky y amy y y y ie draicy ieicy an ricisi adicicavokisifeeic vaficy y anam\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 4100: 3.122817 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.54\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 4200: 3.116419 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.96\n",
      "Validation set perplexity: 19.71\n",
      "Average loss at step 4300: 3.112586 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.70\n",
      "Validation set perplexity: 19.72\n",
      "Average loss at step 4400: 3.071062 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.40\n",
      "Validation set perplexity: 17.66\n",
      "Average loss at step 4500: 3.084864 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.73\n",
      "Validation set perplexity: 19.12\n",
      "Average loss at step 4600: 3.108694 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.09\n",
      "Validation set perplexity: 18.44\n",
      "Average loss at step 4700: 3.128539 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.70\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 4800: 3.110049 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.75\n",
      "Validation set perplexity: 18.63\n",
      "Average loss at step 4900: 3.128833 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.93\n",
      "Validation set perplexity: 19.47\n",
      "Average loss at step 5000: 3.150716 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.20\n",
      "================================================================================\n",
      " by  cliefesy riloraatutloy msy ily asloefikutut cotillelo oy asabuonguty p ahabraotedloy  xitoumeowraloooioloinilitru sedycabe s rclireeddfzqioy eslooo trailms\n",
      "zol d d n rsr d ngngk ded d rssssad n d ntxodss ges tydsr v ngnin n nid rsd ntrmnty r i neusntngd d posnntseneinn d ssd r d n d naner d rkntd d ngchngnld d di r\n",
      "dzexlal esanei banesa e iogo aanuse daatc akau eiaiaise ove icumon ee e anuse ewlae anuil an i jarah tes wuses sb uaataziaesakytoh aos iimesumiaisamumy e  wedan\n",
      "zhosl esoniue eranarelialuc  canumineronb uaiuasdao aranangaere anogniands a mlb ausah aant ine anosi erasria  oahaniniuat f aar ty  a iiais ta csiaemoaany ane \n",
      "yrweon d lar h oruic oesel ty an ty  bnaanon ty  a ti an f oan re  l uin s oonic t f b f tan amsom aific o o a i i f h nay py ou a cia je  w ousuc t din oar o f\n",
      "================================================================================\n",
      "Validation set perplexity: 19.64\n",
      "Average loss at step 5100: 3.066332 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.39\n",
      "Validation set perplexity: 18.16\n",
      "Average loss at step 5200: 3.073208 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.57\n",
      "Validation set perplexity: 17.83\n",
      "Average loss at step 5300: 3.115611 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.87\n",
      "Validation set perplexity: 17.51\n",
      "Average loss at step 5400: 3.103274 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.27\n",
      "Validation set perplexity: 17.35\n",
      "Average loss at step 5500: 3.097294 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.55\n",
      "Validation set perplexity: 17.21\n",
      "Average loss at step 5600: 3.050805 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.85\n",
      "Validation set perplexity: 17.14\n",
      "Average loss at step 5700: 3.048863 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.36\n",
      "Validation set perplexity: 17.14\n",
      "Average loss at step 5800: 3.087459 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.93\n",
      "Validation set perplexity: 16.91\n",
      "Average loss at step 5900: 3.056824 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.38\n",
      "Validation set perplexity: 17.03\n",
      "Average loss at step 6000: 3.064393 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.47\n",
      "================================================================================\n",
      "fva  tnga litoc s laanbad esan ianc  a clem jolyta d ss zeua icaialyzeias  sikua areaz t wl n xoly vkier ls e inchi  sniania datosadetan tauawin mrelipaantis ov\n",
      "sbr anylannen c usy ueazn inisetrkisu atanobn c  rann naloomorisanatx n rin n  cmaoausatoris eaniln  mislaeyr n avn riney n orn n  kn omanatanatnskan e usicanic\n",
      "ab s ciflyia gle elylylylyillylylelylilylylyt lyrolylelelyleillelylylylelylilyarlylylylylylelylylylelylyly alylylylelyitlylelylylylylylylylelilylilylylyt lylyil\n",
      "azony er b ia ara ariauta edicela ak gely ak iinicara y y ary in xivarathiicizonateroness a  iisicc esar oar sedy it hesar hate y e an ra  fo eda iton bediaar i\n",
      "mkonliwh aos sov shldg iuj tak e tatcs hos vanes etiae k r gn re e a tenona  iy  miu aan cy  idr fe  t w i r p br  py ni d hn  athli wtawh a aa  ays a ticnehc m\n",
      "================================================================================\n",
      "Validation set perplexity: 16.93\n",
      "Average loss at step 6100: 3.060265 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.27\n",
      "Validation set perplexity: 16.94\n",
      "Average loss at step 6200: 3.057305 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.97\n",
      "Validation set perplexity: 16.93\n",
      "Average loss at step 6300: 3.011172 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.39\n",
      "Validation set perplexity: 16.66\n",
      "Average loss at step 6400: 3.057981 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.74\n",
      "Validation set perplexity: 16.68\n",
      "Average loss at step 6500: 3.038837 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.09\n",
      "Validation set perplexity: 16.61\n",
      "Average loss at step 6600: 3.040332 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.08\n",
      "Validation set perplexity: 16.81\n",
      "Average loss at step 6700: 3.038986 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.08\n",
      "Validation set perplexity: 16.83\n",
      "Average loss at step 6800: 3.032188 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.19\n",
      "Validation set perplexity: 17.00\n",
      "Average loss at step 6900: 3.020487 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.57\n",
      "Validation set perplexity: 17.17\n",
      "Average loss at step 7000: 3.026023 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.94\n",
      "================================================================================\n",
      "yzou ae e y anauadane eyose ale any ananiee anisy y enesanan tany i y isisdruaaria wan iy e tey e ese ate  re eyany umipiae omangeomane esany y e an aueate alad\n",
      "cpp  f s tly t mly h w m s f o s ba  c t a sla tly ely w aor pst a w t fly j m o alalylyly t ts  i a tdo llylan  p w l h s a elylyly h j c i b plyly d sly wn ly\n",
      "guelane ues e s  aagl ere edeta eresa kao ooane anabanane an pe iae tyare gssmandsa amana  awie at is s sea  tdean a pstan ad ananatansmous ana  banatanlaoseoou\n",
      "cyanat ond suspr c pto s q a i h oar w i t o t d o slepaa  i w m f o a epa l lpa umi o zpo l a t ps  ot  t o sm  msist oteto d m a sst m amo w o p c b ucl t f a\n",
      "ha mraneffgelilerapavip gelsnage dnayigebithnancno d tysthge igegerananontserenes vages radagicy m gntdurkctncthreradrrylbgentneglgelas menozasegebez grchgey ra\n",
      "================================================================================\n",
      "Validation set perplexity: 16.94\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = np.argmax(batches[i], -1)\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          arg_feed = np.argmax(feed)\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            \n",
    "            prediction = sample_prediction.eval({sample_input: [arg_feed]})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: [np.argmax(b[0])]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 4, 31)\n",
      "('a', 'z', ' ', '!')\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 35#len(string.ascii_lowercase) + 2 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 5\n",
    "  elif char == ' ':\n",
    "    return 4\n",
    "  elif char == '!':\n",
    "    return 31\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid == 31 :\n",
    "    return '!'\n",
    "  elif dictid > 4:\n",
    "    return chr(dictid + first_letter - 5)\n",
    "  elif dictid == 4:\n",
    "    return ' '\n",
    "  else:\n",
    "    return '@'\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('!'))\n",
    "print(id2char(5), id2char(30), id2char(4),id2char(31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=19\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def ids(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, ids(b))]\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward=[]\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1]+' '\n",
    "    return map(lambda x: char2id(x), backward[:-1]+['!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(sess, forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size = vocabulary_size,\n",
    "                                     target_vocab_size = vocabulary_size,\n",
    "                                     buckets = [(20, 21)],\n",
    "                                     size = 64,\n",
    "                                     num_layers = 1, # one encoding and one decoding LSTM\n",
    "                                     max_gradient_norm = 2.0,\n",
    "                                     batch_size = batch_size,\n",
    "                                     learning_rate = 0.1,\n",
    "                                     learning_rate_decay_factor = 0.9,\n",
    "                                     forward_only = forward_only)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 500 learning rate 0.1000 step-time 0.05 perplexity 15.69\n",
      "('## : ', [9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'e                    ')\n",
      "global step 1000 learning rate 0.1000 step-time 0.04 perplexity 7.97\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 1500 learning rate 0.1000 step-time 0.04 perplexity 2.78\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 2000 learning rate 0.1000 step-time 0.04 perplexity 1.55\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 2500 learning rate 0.1000 step-time 0.04 perplexity 1.22\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 3000 learning rate 0.1000 step-time 0.04 perplexity 1.11\n",
      "('## : ', [9, 9, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eee                  ')\n",
      "global step 3500 learning rate 0.1000 step-time 0.04 perplexity 1.08\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 4000 learning rate 0.1000 step-time 0.04 perplexity 1.05\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 4500 learning rate 0.1000 step-time 0.04 perplexity 1.05\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 5000 learning rate 0.1000 step-time 0.04 perplexity 1.04\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 5500 learning rate 0.1000 step-time 0.04 perplexity 1.03\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 6000 learning rate 0.1000 step-time 0.04 perplexity 1.03\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 6500 learning rate 0.1000 step-time 0.04 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 7000 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 7500 learning rate 0.1000 step-time 0.04 perplexity 1.02\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 8000 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 9, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'ehe                  ')\n",
      "global step 8500 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 9000 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 9500 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 10000 learning rate 0.1000 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 10500 learning rate 0.1000 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 11000 learning rate 0.1000 step-time 0.05 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 11500 learning rate 0.1000 step-time 0.04 perplexity 1.02\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 12000 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 12500 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 13000 learning rate 0.0900 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 13500 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 14000 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 14500 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 15000 learning rate 0.0900 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 15500 learning rate 0.0900 step-time 0.04 perplexity 1.01\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 16000 learning rate 0.0810 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 16500 learning rate 0.0810 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 17000 learning rate 0.0810 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 17500 learning rate 0.0810 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 18000 learning rate 0.0729 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 18500 learning rate 0.0729 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 19000 learning rate 0.0729 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 19500 learning rate 0.0729 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 20000 learning rate 0.0729 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 20500 learning rate 0.0729 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 21000 learning rate 0.0656 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 21500 learning rate 0.0656 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 22000 learning rate 0.0656 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 22500 learning rate 0.0656 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 23000 learning rate 0.0656 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 23500 learning rate 0.0590 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 24000 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 24500 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 25000 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 25500 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 26000 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 26500 learning rate 0.0590 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 27000 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 27500 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 28000 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 28500 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 29000 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 29500 learning rate 0.0531 step-time 0.04 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "global step 30000 learning rate 0.0531 step-time 0.05 perplexity 1.00\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('>>>>>>>>> ', 'the quick brown fox', ' -> ', 'eht                  ')\n",
      "()\n",
      "()\n",
      "('## : ', [9, 12, 24, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "('the quick brown fox', ' -> ', 'eht                  ')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = create_model(sess, False)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_steps = 30001\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 500\n",
    "    valid_ckpt = 500\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "      model.batch_size = batch_size\n",
    "      batches = batches2string(train_batches.next())\n",
    "      train_sets = []\n",
    "      batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "      batch_decs = map(lambda x: rev_id(x), batches)\n",
    "      for i in range(len(batch_encs)):\n",
    "        train_sets.append((batch_encs[i],batch_decs[i]))\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "      \n",
    "      step_time += (time.time() - start_time) / step_ckpt\n",
    "      loss += step_loss / step_ckpt\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if step % step_ckpt == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "\n",
    "        step_time, loss = 0.0, 0.0\n",
    "\n",
    "        \n",
    "        if step % valid_ckpt == 0:\n",
    "            v_loss = 0.0\n",
    "            \n",
    "            model.batch_size = 1 \n",
    "            batches = ['the quick brown fox']\n",
    "            test_sets = []\n",
    "            batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "            #batch_decs = map(lambda x: rev_id(x), batches)\n",
    "            test_sets.append((batch_encs[0],[]))\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "            \n",
    "            \n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            print ('## : ', outputs)\n",
    "\n",
    "            if char2id('!') in outputs:\n",
    "                outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "            print('>>>>>>>>> ',batches[0], ' -> ' ,''.join(map(lambda x: id2char(x),outputs)))\n",
    "            sys.stdout.flush()\n",
    "            '''\n",
    "            \n",
    "            for _ in range(valid_size):\n",
    "                model.batch_size = 1\n",
    "                v_batches = batches2string(valid_batches.next())\n",
    "                valid_sets = []\n",
    "                v_batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),v_batches)\n",
    "                v_batch_decs = map(lambda x: rev_id(x), v_batches)\n",
    "                for i in range(len(v_batch_encs)):\n",
    "                  valid_sets.append((v_batch_encs[i],v_batch_decs[i]))\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                v_loss += eval_loss / valid_size\n",
    "\n",
    "            eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "            print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "            sys.stdout.flush()\n",
    "            '''\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = map(lambda x:map(lambda y: char2id(y), list(x)),batches)\n",
    "    #batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0],[]))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print ('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "    \n",
    "    print(batches[0], ' -> ' ,''.join(map(lambda x: id2char(x),outputs)))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
